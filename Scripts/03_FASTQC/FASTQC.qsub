#!/bin/bash -l

set -o errexit
set -o pipefail
set -o nounset

##################################################################################
#Andy Rampersaud, 02.23.16
#This script is called by fastqC.sh
##################################################################################
# Specify which shell to use
#$ -S /bin/bash
# current working directory option
#$ -cwd
#$ -j y
#$ -e ./logs/
#$ -o ./logs/
#$ -pe omp 2
#$ -l mem_per_core=4G
##################################################################################
#Initialize variables from fastqC.sh
##################################################################################

# export all variables from Pipeline_Setup.conf
eval "$(../00_Setup_Pipeline/01_Pipeline_Setup.py --export)"

if [ $# -ne 2 ]; then
    echo "Need 2 arguments for the qsub command:"
    echo "qsub -N ${job_name}_${sample_id} -P ${PROJECT} -l h_rt=${TIME_LIMIT} FASTQC.qsub ${sample_id} ${output_dir}"
    exit 1
fi

#process the command line arguments
sample_id=$1
output_dir=$2

# Now let's keep track of some information just in case anything goes wrong
echo "=========================================================="
#Use to calculate job time:
#Start_Time in seconds
start_time=$(date +"%s")
echo "Starting on : $(date)"
echo "Running on node : $(hostname)"
echo "Current directory : $(pwd)"
echo "Current job ID : $JOB_ID"
echo "Current job name : $JOB_NAME"
echo "Task index number : $SGE_TASK_ID"
echo "Parameter for multiple cores : $NSLOTS"
echo "=========================================================="

# Go to local scratch directory

printf "\nChange dir to scratch directory\n"
cd ${TMPDIR}
printf "\nPrint scratch directory location: ${TMPDIR}"
printf "\nLoading required modules...\n"

set +eu
module load miniconda
# conda activate --stack ${CONDA_DIR}/fastqc
conda activate --stack ${CONDA_DIR}/falco
set -eu

# get_sample_info return (PRJ_NAME, READ1, READ2) or EXCEPTION
sample_info=($("${SETUP_PIPELINE_DIR}"/01_Pipeline_Setup.py --get_sample_info ${sample_id}))

## it will work for PAIRED/SINGLE-END data
## If R1 == R2 -- single-end data
R1=${sample_info[1]}
R2=${sample_info[2]}

# if fastq files do not exist the following commands should raise
# an error and script will be interrupted
# (for SINGLE_END R1==R2)
for f in $R1; do
    [ -e "$f" ] && echo "R1 file $f exists" || (
        echo "ERROR: R1 file $f does not exist"
        exit 1
    )
done

for f in $R2; do
    [ -e "$f" ] && echo "R2 file $f exists" || (
        echo "ERROR: R2 file $f does not exist"
        exit 1
    )
done

## Run FASTP only for paired-end data
## Run FASTQC/FALCO twice for R1 and R2 separatelly
if [[ ${R1} != "${R2}" ]]; then

    ## PAIRED-END
    ## FASTP
    fastp_dir="${sample_id}_FASTP"
    mkdir -p "${fastp_dir}"

    pushd "${fastp_dir}"
    fastp -i ${R1} -I ${R2}
    popd

    cp -r $fastp_dir $output_dir

    ## FASTQC/FALCO R1
    fastq_dir="${sample_id}_FASTQC_R1"
    mkdir -p ${fastq_dir}
    (
        set -x
        falco --quiet --outdir ${fastq_dir} $R1
    )
    cp -r ${fastq_dir} ${output_dir}

    ## separately FASTQC/FALCO R2
    fastq_dir="${sample_id}_FASTQC_R2"
    mkdir -p ${fastq_dir}
    (
        set -x
        falco --quiet --outdir ${fastq_dir} $R2
    )
    cp -r ${fastq_dir} ${output_dir}
else
    ## SINGLE-END
    ## FASTQC/FALCO (ONLY R1 for single-end)
    fastq_dir="${sample_id}_FASTQC_R1"
    mkdir -p ${fastq_dir}
    (
        set -x
        falco --quiet --outdir ${fastq_dir} $R1
    )
    cp -r ${fastq_dir} ${output_dir}
fi

ls -alh

echo "Finished on : $(date)"

#Use to calculate job time:
#End_Time in seconds
end_time=$(date +"%s")
diff=$((end_time - start_time))
echo "$((diff / 3600)) hours, $(((diff / 60) % 60)) minutes and $((diff % 60)) seconds elapsed."
echo "=========================================================="
echo "IAMOK"
